Here’s how to structure the complete project that integrates FastAPI with a PostgreSQL database, mocked S3 using MinIO, and Prometheus for monitoring. This includes all necessary files and configurations.

### Project Structure

```plaintext
your_project/
├── alembic/
│   ├── env.py
│   ├── script.py.mako
│   └── versions/
├── alembic.ini
├── app/
│   ├── __init__.py
│   ├── database.py
│   ├── models.py
│   ├── server.py
│   ├── model_registry.py
│   ├── download_tracker.py
│   └── client.py
├── Dockerfile
├── docker-compose.yml
├── prometheus.yml
└── requirements.txt
```

### 1. **Docker Compose Configuration (`docker-compose.yml`)**

```yaml
version: '3.8'

services:
  app:
    build: .
    container_name: fastapi_app
    ports:
      - "8001:8001"
    environment:
      - DATABASE_URL=postgresql+asyncpg://model_user:yourpassword@db/model_db
      - MINIO_ENDPOINT=http://minio:9000
      - MINIO_ACCESS_KEY=admin
      - MINIO_SECRET_KEY=password
    depends_on:
      - db
      - minio
      - prometheus

  db:
    image: postgres:13
    container_name: postgres_db
    environment:
      POSTGRES_USER: model_user
      POSTGRES_PASSWORD: yourpassword
      POSTGRES_DB: model_db
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  minio:
    image: quay.io/minio/minio
    container_name: minio
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    volumes:
      - grafana_data:/var/lib/grafana

volumes:
  postgres_data:
  grafana_data:
```

### 2. **Dockerfile**

```dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --upgrade pip
RUN pip install -r requirements.txt

COPY . .

EXPOSE 8001

CMD ["python", "app/server.py"]
```

### 3. **Python Dependencies (`requirements.txt`)**

```plaintext
fastapi
uvicorn
sqlalchemy[asyncpg]
alembic
asyncpg
boto3
requests
prometheus_client
```

### 4. **Alembic Configuration (`alembic.ini`)**

Ensure that the `sqlalchemy.url` is correctly set in `alembic.ini`:

```ini
# alembic.ini
[alembic]
script_location = alembic

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname = 

[logger_sqlalchemy]
level = WARN
handlers = console
qualname = sqlalchemy.engine
# ... rest of the config

sqlalchemy.url = postgresql+asyncpg://model_user:yourpassword@db/model_db
```

### 5. **Database Models (`app/models.py`)**

```python
from sqlalchemy import Column, Integer, String, Float, Text
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class Model(Base):
    __tablename__ = 'models'

    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, unique=True, index=True)
    user = Column(String)
    project_name = Column(String)
    bucket_url = Column(String)
    key = Column(String)

class DownloadLog(Base):
    __tablename__ = 'download_logs'

    id = Column(Integer, primary_key=True, index=True)
    engineer = Column(String)
    model_name = Column(String)
    project_name = Column(String)
    size_bytes = Column(Integer)
    download_time_seconds = Column(Float)
    timestamp = Column(Float)
```

### 6. **Database Initialization and Session Management (`app/database.py`)**

```python
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker
from app.models import Base

DATABASE_URL = "postgresql+asyncpg://model_user:yourpassword@db/model_db"

engine = create_async_engine(DATABASE_URL, echo=True)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine, class_=AsyncSession)

async def init_db():
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

async def get_db():
    async with SessionLocal() as session:
        yield session
```

### 7. **Model Registration and Management (`app/model_registry.py`)**

```python
from sqlalchemy.future import select
from sqlalchemy.exc import IntegrityError
from sqlalchemy.ext.asyncio import AsyncSession
from app.models import Model

class ModelRegistry:
    def __init__(self, db: AsyncSession):
        self.db = db

    async def register_model(self, model_data: dict):
        new_model = Model(**model_data)
        self.db.add(new_model)
        try:
            await self.db.commit()
            await self.db.refresh(new_model)
            return new_model.id
        except IntegrityError:
            await self.db.rollback()
            raise ValueError(f"Model with name {model_data['name']} is already registered.")

    async def get_model(self, model_name: str) -> dict:
        result = await self.db.execute(select(Model).where(Model.name == model_name))
        model = result.scalars().first()
        if not model:
            raise ValueError(f"Model with name {model_name} not found.")
        return model

    async def list_models(self):
        result = await self.db.execute(select(Model))
        return result.scalars().all()
```

### 8. **Download Tracker (`app/download_tracker.py`)**

```python
import boto3
import os
import time
from sqlalchemy.ext.asyncio import AsyncSession
from app.models import DownloadLog

class DownloadTracker:
    def __init__(self, db: AsyncSession):
        self.db = db
        self.s3 = boto3.client(
            's3',
            endpoint_url=os.getenv('MINIO_ENDPOINT', 'http://localhost:9000'),
            aws_access_key_id=os.getenv('MINIO_ACCESS_KEY', 'admin'),
            aws_secret_access_key=os.getenv('MINIO_SECRET_KEY', 'password')
        )

    async def log_download_event(self, engineer: str, model: dict, model_size: int, download_time: float):
        log_entry = DownloadLog(
            engineer=engineer,
            model_name=model.name,
            project_name=model.project_name,
            size_bytes=model_size,
            download_time_seconds=download_time,
            timestamp=time.time()
        )
        self.db.add(log_entry)
        await self.db.commit()

    async def download_model(self, model_name: str) -> str:
        model = await self.db.get_model(model_name)
        bucket_url = model.bucket_url
        key = model.key
        bucket_name = bucket_url.replace("s3://", "").split('/')[0]
        model_path = f"/tmp/{model_name}"
        self.s3.download_file(bucket_name, key, model_path)
        return model_path

    def get_model_size(self, model_path: str) -> int:
        return os.path.getsize(model_path)

    def clean_up(self, model_path: str):
        if os.path.exists(model_path):
            os.remove(model_path)
```

### 9. **FastAPI Application (`app/server.py`)**

```python
from fastapi import FastAPI, HTTPException, Request, Depends
from fastapi.responses import FileResponse
from sqlalchemy.ext.asyncio import AsyncSession
from app.database import get_db, init_db
from app.model_registry import ModelRegistry
from app.download_tracker import DownloadTracker
import uvicorn
from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST

app = FastAPI(on_startup=[init_db])

model_registration_counter = Counter('model_registrations_total', 'Total number of model registrations')
model_download_counter = Counter('model_downloads_total', 'Total number of model downloads')
model_download_time_histogram = Histogram('model_download_time_seconds', 'Time spent downloading a model')

@app.post("/register_model/")
async def register_model(model: dict, db: AsyncSession = Depends(get_db)):
    registry = ModelRegistry(db)
    try:
        model_id = await registry.register_model(model)
        model_registration_counter.inc()
        return {"message": f"Model '{model['name']}' registered successfully.", "id": model_id}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get("/download/{model_name}")
async def download_model(model_name: str, request: Request, db: AsyncSession = Depends(get_db)):
    tracker = DownloadTracker(db)
    engineer = request

.headers.get('X-Engineer-ID', 'Unknown')
    model_path = None
    try:
        with model_download_time_histogram.time():
            model_path = await tracker.download_model(model_name)
            model_size = tracker.get_model_size(model_path)
            model = await tracker.db.get_model(model_name)
            await tracker.log_download_event(engineer, model, model_size, time.time())
            model_download_counter.inc()
        return FileResponse(model_path, media_type='application/octet-stream', filename=model_name)
    except Exception as e:
        raise HTTPException(status_code=404, detail=f"Error downloading model: {str(e)}")
    finally:
        if model_path:
            tracker.clean_up(model_path)

@app.get("/list_models/")
async def list_models(db: AsyncSession = Depends(get_db)):
    registry = ModelRegistry(db)
    models = await registry.list_models()
    return {"models": models}

@app.get("/metrics")
async def metrics():
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001)
```

### 10. **Prometheus Configuration (`prometheus.yml`)**

```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'fastapi_app'
    static_configs:
      - targets: ['app:8001']
```

### 11. **Client Script (`app/client.py`)**

```python
import requests
import os
import logging

class ModelClient:
    def __init__(self, server_url: str, engineer_id: str):
        self.server_url = server_url
        self.engineer_id = engineer_id
        logging.basicConfig(level=logging.INFO)

    def register_model(self, name: str, user: str, project_name: str, bucket_url: str, key: str):
        model_data = {
            "name": name,
            "user": user,
            "project_name": project_name,
            "bucket_url": bucket_url,
            "key": key
        }
        response = requests.post(f"{self.server_url}/register_model/", json=model_data)
        if response.status_code == 200:
            logging.info(f"Model '{name}' registered successfully. ID: {response.json().get('id')}")
        else:
            logging.error(f"Failed to register model '{name}': {response.text}")

    def download_model(self, model_name: str, save_path: str):
        headers = {'X-Engineer-ID': self.engineer_id}
        response = requests.get(f"{self.server_url}/download/{model_name}", headers=headers, stream=True)
        
        if response.status_code == 200:
            os.makedirs(save_path, exist_ok=True)
            file_path = os.path.join(save_path, model_name)
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            logging.info(f"Model '{model_name}' downloaded and saved to '{file_path}'.")
        else:
            logging.error(f"Failed to download model '{model_name}': {response.text}")

    def list_models(self):
        response = requests.get(f"{self.server_url}/list_models/")
        if response.status_code == 200:
            models = response.json().get('models', [])
            logging.info(f"Registered models: {models}")
            return models
        else:
            logging.error(f"Failed to list models: {response.text}")
            return []
```

### 12. **Running the Project**

1. **Build and Start Services:**
   ```bash
   docker-compose up --build
   ```

2. **Access Services:**
   - **FastAPI App:** `http://localhost:8001`
   - **MinIO Console:** `http://localhost:9001`
   - **Prometheus:** `http://localhost:9090`
   - **Grafana:** `http://localhost:3000`

3. **Client Usage:**
   Use the `ModelClient` in your Python scripts to interact with the FastAPI app.

This complete setup allows you to run the FastAPI application with a PostgreSQL database, MinIO for S3-compatible storage, Prometheus for monitoring, and Grafana for visualizing metrics, all orchestrated using Docker Compose.
